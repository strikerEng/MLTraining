digraph "classes_train_structure" {
rankdir=BT
charset="utf-8"
"olmo.train.BatchStatsMonitor" [color="#77AADD", fontcolor="black", label=<{BatchStatsMonitor|max_window_size : int<br ALIGN="LEFT"/>sync_nodes : bool<br ALIGN="LEFT"/>|check(device)<br ALIGN="LEFT"/>log_batch(batch)<br ALIGN="LEFT"/>reset(): None<br ALIGN="LEFT"/>}>, shape="record", style="filled"];
"olmo.train.DatasetMetrics" [color="#77AADD", fontcolor="black", label=<{DatasetMetrics|eval_loader<br ALIGN="LEFT"/>eval_metric : Union[Metric, Dict[str, Metric], List[Metric]]<br ALIGN="LEFT"/>label : str<br ALIGN="LEFT"/>subset_num_batches : Optional[int]<br ALIGN="LEFT"/>|compute_metrics(): Dict[str, float]<br ALIGN="LEFT"/>reset_metrics(): None<br ALIGN="LEFT"/>update_metrics(batch: Dict[str, Any], eval_out: Dict[str, torch.Tensor]): None<br ALIGN="LEFT"/>}>, shape="record", style="filled"];
"olmo.train.LRMonitor" [color="#77AADD", fontcolor="black", label=<{LRMonitor|optim<br ALIGN="LEFT"/>|check(): Dict[str, float]<br ALIGN="LEFT"/>}>, shape="record", style="filled"];
"olmo.train.SpeedMonitor" [color="#77AADD", fontcolor="black", label=<{SpeedMonitor|cfg : SpeedMonitorConfig<br ALIGN="LEFT"/>global_total_tokens : int<br ALIGN="LEFT"/>stats : Deque[Tuple[float, int, int]]<br ALIGN="LEFT"/>|batch_start(global_total_tokens: int, device_batch_num_tokens: int, device_batch_num_loss_tokens: int, record: bool): None<br ALIGN="LEFT"/>check(): Dict[str, float]<br ALIGN="LEFT"/>reset(): None<br ALIGN="LEFT"/>}>, shape="record", style="filled"];
"olmo.train.Trainer" [color="#77AADD", fontcolor="black", label=<{Trainer|batches_per_epoch<br ALIGN="LEFT"/>checkpoints : List[Path]<br ALIGN="LEFT"/>cur_train_loss : float<br ALIGN="LEFT"/>dataset<br ALIGN="LEFT"/>device<br ALIGN="LEFT"/>ephemeral_checkpoints : List[Path]<br ALIGN="LEFT"/>epoch : Optional[int]<br ALIGN="LEFT"/>evaluators : List[DatasetMetrics]<br ALIGN="LEFT"/>fsdp_model<br ALIGN="LEFT"/>global_step : int<br ALIGN="LEFT"/>global_train_examples_seen_this_epoch : int<br ALIGN="LEFT"/>global_train_tokens_seen : int<br ALIGN="LEFT"/>inference_evaluators : List[InfDatasetEvaluator]<br ALIGN="LEFT"/>last_sharded_checkpoint_step : Optional[int]<br ALIGN="LEFT"/>last_unsharded_checkpoint_step : Optional[int]<br ALIGN="LEFT"/>loss_fn : Callable[..., torch.Tensor]<br ALIGN="LEFT"/>max_epochs<br ALIGN="LEFT"/>max_steps<br ALIGN="LEFT"/>max_tokens<br ALIGN="LEFT"/>min_train_loss : float<br ALIGN="LEFT"/>model : Molmo<br ALIGN="LEFT"/>moe_args<br ALIGN="LEFT"/>optim : Optimizer<br ALIGN="LEFT"/>scheduler : Scheduler<br ALIGN="LEFT"/>scheduler_current<br ALIGN="LEFT"/>scheduler_max<br ALIGN="LEFT"/>tokens_per_batch<br ALIGN="LEFT"/>train_loader<br ALIGN="LEFT"/>training_config : TrainConfig<br ALIGN="LEFT"/>unsharded_checkpoints : List[Path]<br ALIGN="LEFT"/>|check_if_cancelled(): Tuple[bool, int]<br ALIGN="LEFT"/>close(exit_code: int): None<br ALIGN="LEFT"/>eval(): Dict[str, Any]<br ALIGN="LEFT"/>eval_batch(batch: Dict[str, Any]): Tuple[torch.Tensor, torch.Tensor]<br ALIGN="LEFT"/>eval_step(batch: Dict[str, Any], evaluator: DatasetMetrics): None<br ALIGN="LEFT"/>fit()<br ALIGN="LEFT"/>get_labels(batch: Dict[str, Any]): torch.Tensor<br ALIGN="LEFT"/>inference_eval(): Dict[str, Union[float, WBValue]]<br ALIGN="LEFT"/>load_trainer_state_dict(state_dict: Dict[str, Any]): None<br ALIGN="LEFT"/>log_metrics_to_console(prefix: str, metrics: Dict[str, float])<br ALIGN="LEFT"/>model_forward(batch: Dict[str, Any], loss_reduction: str, compute_z_loss: bool): Tuple[torch.Tensor, torch.Tensor, Optional[torch.Tensor], torch.Tensor]<br ALIGN="LEFT"/>move_to_device(batch, device)<br ALIGN="LEFT"/>remove_checkpoint(idx: int, checkpoint_type: CheckpointType)<br ALIGN="LEFT"/>remove_ephemeral_checkpoint(idx: int)<br ALIGN="LEFT"/>remove_sharded_checkpoint(idx: int)<br ALIGN="LEFT"/>remove_unsharded_checkpoint(idx: int)<br ALIGN="LEFT"/>restore_checkpoint(load_path: PathOrStr)<br ALIGN="LEFT"/>restore_rng_state(rng_state: Dict[str, Any]): None<br ALIGN="LEFT"/>restore_sharded_checkpoint(load_path: PathOrStr, local_cache: Optional[PathOrStr])<br ALIGN="LEFT"/>restore_unsharded_checkpoint(load_path: PathOrStr, local_cache: Optional[PathOrStr])<br ALIGN="LEFT"/>save_checkpoint(checkpoint_type: CheckpointType): Tuple[PathOrStr, Optional[PathOrStr]]<br ALIGN="LEFT"/>save_ephemeral_checkpoint(): Tuple[PathOrStr, Optional[PathOrStr]]<br ALIGN="LEFT"/>save_sharded_checkpoint(): Tuple[PathOrStr, Optional[PathOrStr]]<br ALIGN="LEFT"/>save_unsharded_checkpoint(): Tuple[PathOrStr, Optional[PathOrStr]]<br ALIGN="LEFT"/>should_log_optim_metrics_this_step(): bool<br ALIGN="LEFT"/>should_log_this_step(): bool<br ALIGN="LEFT"/>split_batch(batch: Dict[str, Any]): List[Dict[str, Any]]<br ALIGN="LEFT"/>system_metrics(): Dict[str, float]<br ALIGN="LEFT"/>train_batch(batch: Dict[str, Any]): Tuple[torch.Tensor, Optional[torch.Tensor], torch.Tensor]<br ALIGN="LEFT"/>train_step(batch: Dict[str, Any], reduce_global_loss: bool): Dict[str, float]<br ALIGN="LEFT"/>trainer_state_dict(): Dict[str, Any]<br ALIGN="LEFT"/>}>, shape="record", style="filled"];
}
